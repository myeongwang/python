{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0771c984",
   "metadata": {},
   "source": [
    "1. define X,y\n",
    "2. split train & validation dataset\n",
    "3. modeling\n",
    "4. model 해석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40c1b3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import re\n",
    "import pickle\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "from lightgbm import LGBMClassifier, LGBMRegressor\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44f3e8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loading (수술 時 사망 데이터)\n",
    "data=pd.read_csv(\"https://raw.githubusercontent.com/GonieAhn/Data-Science-online-course-from-gonie/main/Data%20Store/example_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2eff4540",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>censor</th>\n",
       "      <th>event</th>\n",
       "      <th>age</th>\n",
       "      <th>wtkg</th>\n",
       "      <th>hemo</th>\n",
       "      <th>homo</th>\n",
       "      <th>drugs</th>\n",
       "      <th>karnof</th>\n",
       "      <th>oprior</th>\n",
       "      <th>z30</th>\n",
       "      <th>...</th>\n",
       "      <th>gender</th>\n",
       "      <th>str2</th>\n",
       "      <th>strat</th>\n",
       "      <th>symptom</th>\n",
       "      <th>cd40</th>\n",
       "      <th>cd420</th>\n",
       "      <th>cd496</th>\n",
       "      <th>r</th>\n",
       "      <th>cd80</th>\n",
       "      <th>cd820</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>532.000000</td>\n",
       "      <td>532.000000</td>\n",
       "      <td>532.000000</td>\n",
       "      <td>532.000000</td>\n",
       "      <td>532.000000</td>\n",
       "      <td>532.000000</td>\n",
       "      <td>532.000000</td>\n",
       "      <td>532.000000</td>\n",
       "      <td>532.000000</td>\n",
       "      <td>532.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>532.000000</td>\n",
       "      <td>532.000000</td>\n",
       "      <td>532.000000</td>\n",
       "      <td>532.000000</td>\n",
       "      <td>532.000000</td>\n",
       "      <td>532.000000</td>\n",
       "      <td>532.000000</td>\n",
       "      <td>532.000000</td>\n",
       "      <td>532.000000</td>\n",
       "      <td>532.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.340226</td>\n",
       "      <td>801.236842</td>\n",
       "      <td>35.225564</td>\n",
       "      <td>76.061855</td>\n",
       "      <td>0.078947</td>\n",
       "      <td>0.640977</td>\n",
       "      <td>0.118421</td>\n",
       "      <td>95.432331</td>\n",
       "      <td>0.030075</td>\n",
       "      <td>0.546992</td>\n",
       "      <td>...</td>\n",
       "      <td>0.812030</td>\n",
       "      <td>0.580827</td>\n",
       "      <td>1.981203</td>\n",
       "      <td>0.167293</td>\n",
       "      <td>353.204887</td>\n",
       "      <td>336.139098</td>\n",
       "      <td>173.146617</td>\n",
       "      <td>0.603383</td>\n",
       "      <td>987.250000</td>\n",
       "      <td>928.214286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.474231</td>\n",
       "      <td>326.887929</td>\n",
       "      <td>8.852094</td>\n",
       "      <td>13.224698</td>\n",
       "      <td>0.269910</td>\n",
       "      <td>0.480165</td>\n",
       "      <td>0.323410</td>\n",
       "      <td>5.981856</td>\n",
       "      <td>0.170955</td>\n",
       "      <td>0.498255</td>\n",
       "      <td>...</td>\n",
       "      <td>0.391056</td>\n",
       "      <td>0.493888</td>\n",
       "      <td>0.905946</td>\n",
       "      <td>0.373589</td>\n",
       "      <td>114.105253</td>\n",
       "      <td>130.961573</td>\n",
       "      <td>191.455406</td>\n",
       "      <td>0.489656</td>\n",
       "      <td>475.223907</td>\n",
       "      <td>438.569798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>47.401000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>103.000000</td>\n",
       "      <td>49.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>221.000000</td>\n",
       "      <td>150.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>535.750000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>67.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>271.000000</td>\n",
       "      <td>243.750000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>653.250000</td>\n",
       "      <td>626.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>933.500000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>74.600000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>346.000000</td>\n",
       "      <td>330.500000</td>\n",
       "      <td>113.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>881.000000</td>\n",
       "      <td>818.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1081.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>83.502000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>422.000000</td>\n",
       "      <td>418.000000</td>\n",
       "      <td>324.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1190.000000</td>\n",
       "      <td>1164.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1231.000000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>149.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>771.000000</td>\n",
       "      <td>909.000000</td>\n",
       "      <td>857.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4255.000000</td>\n",
       "      <td>3130.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           censor        event         age        wtkg        hemo  \\\n",
       "count  532.000000   532.000000  532.000000  532.000000  532.000000   \n",
       "mean     0.340226   801.236842   35.225564   76.061855    0.078947   \n",
       "std      0.474231   326.887929    8.852094   13.224698    0.269910   \n",
       "min      0.000000    33.000000   13.000000   47.401000    0.000000   \n",
       "25%      0.000000   535.750000   29.000000   67.500000    0.000000   \n",
       "50%      0.000000   933.500000   34.000000   74.600000    0.000000   \n",
       "75%      1.000000  1081.000000   40.000000   83.502000    0.000000   \n",
       "max      1.000000  1231.000000   70.000000  149.000000    1.000000   \n",
       "\n",
       "             homo       drugs      karnof      oprior         z30  ...  \\\n",
       "count  532.000000  532.000000  532.000000  532.000000  532.000000  ...   \n",
       "mean     0.640977    0.118421   95.432331    0.030075    0.546992  ...   \n",
       "std      0.480165    0.323410    5.981856    0.170955    0.498255  ...   \n",
       "min      0.000000    0.000000   70.000000    0.000000    0.000000  ...   \n",
       "25%      0.000000    0.000000   90.000000    0.000000    0.000000  ...   \n",
       "50%      1.000000    0.000000  100.000000    0.000000    1.000000  ...   \n",
       "75%      1.000000    0.000000  100.000000    0.000000    1.000000  ...   \n",
       "max      1.000000    1.000000  100.000000    1.000000    1.000000  ...   \n",
       "\n",
       "           gender        str2       strat     symptom        cd40       cd420  \\\n",
       "count  532.000000  532.000000  532.000000  532.000000  532.000000  532.000000   \n",
       "mean     0.812030    0.580827    1.981203    0.167293  353.204887  336.139098   \n",
       "std      0.391056    0.493888    0.905946    0.373589  114.105253  130.961573   \n",
       "min      0.000000    0.000000    1.000000    0.000000  103.000000   49.000000   \n",
       "25%      1.000000    0.000000    1.000000    0.000000  271.000000  243.750000   \n",
       "50%      1.000000    1.000000    2.000000    0.000000  346.000000  330.500000   \n",
       "75%      1.000000    1.000000    3.000000    0.000000  422.000000  418.000000   \n",
       "max      1.000000    1.000000    3.000000    1.000000  771.000000  909.000000   \n",
       "\n",
       "            cd496           r         cd80        cd820  \n",
       "count  532.000000  532.000000   532.000000   532.000000  \n",
       "mean   173.146617    0.603383   987.250000   928.214286  \n",
       "std    191.455406    0.489656   475.223907   438.569798  \n",
       "min     -1.000000    0.000000   221.000000   150.000000  \n",
       "25%     -1.000000    0.000000   653.250000   626.500000  \n",
       "50%    113.000000    1.000000   881.000000   818.000000  \n",
       "75%    324.000000    1.000000  1190.000000  1164.000000  \n",
       "max    857.000000    1.000000  4255.000000  3130.000000  \n",
       "\n",
       "[8 rows x 23 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe() #censor is target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "121ffc8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>> Data Shape : (532, 22)\n"
     ]
    }
   ],
   "source": [
    "# Data Quality Checking\n",
    "col = []\n",
    "missing = []\n",
    "level = [] \n",
    "for name in data.columns:\n",
    "    \n",
    "    # Missing\n",
    "    missper = data[name].isnull().sum() / data.shape[0]\n",
    "    missing.append(round(missper, 4))\n",
    "\n",
    "    # Leveling\n",
    "    lel = data[name].dropna()\n",
    "    level.append(len(list(set(lel))))\n",
    "\n",
    "    # Columns\n",
    "    col.append(name)\n",
    "\n",
    "summary = pd.concat([pd.DataFrame(col, columns=['name']), \n",
    "                     pd.DataFrame(missing, columns=['Missing Percentage']), \n",
    "                     pd.DataFrame(level, columns=['Level'])], axis=1)\n",
    "\n",
    "drop_col = summary['name'][(summary['Level'] <= 1) | (summary['Missing Percentage'] >= 0.8)]\n",
    "data.drop(columns=drop_col, inplace=True)\n",
    "print(\">>>> Data Shape : {}\".format(data.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1ec7481",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10    zprior\n",
       "Name: name, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drop_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a03cacba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X's & Y Split\n",
    "Y = data['censor']\n",
    "X = data.drop(columns=['censor'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d205b66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>> # of Train data : 372\n",
      ">>>> # of valid data : 160\n",
      ">>>> # of Train data Y : Counter({0: 241, 1: 131})\n",
      ">>>> # of valid data Y : Counter({0: 110, 1: 50})\n"
     ]
    }
   ],
   "source": [
    "idx = list(range(X.shape[0]))\n",
    "train_idx, valid_idx = train_test_split(idx, test_size=0.3, random_state=2021)\n",
    "print(\">>>> # of Train data : {}\".format(len(train_idx)))\n",
    "print(\">>>> # of valid data : {}\".format(len(valid_idx)))\n",
    "print(\">>>> # of Train data Y : {}\".format(Counter(Y.iloc[train_idx])))\n",
    "print(\">>>> # of valid data Y : {}\".format(Counter(Y.iloc[valid_idx])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d745710",
   "metadata": {},
   "source": [
    "[LightGBM Parameters]\n",
    "  - Package : https://lightgbm.readthedocs.io/en/latest/Python-Intro.html\n",
    "  - learning_rate : GBM에서 shrinking 하는 것과 같은 것\n",
    "  - reg_lambda : L2 regularization term on weights (analogous to Ridge regression)\n",
    "  - reg_alpha : L1 regularization term on weight (analogous to Lasso regression)\n",
    "  - objective \n",
    "        objective 🔗︎, default = regression, type = enum, options: regression, regression_l1, huber, fair, poisson, quantile, mape, gamma, tweedie, binary, multiclass, multiclassova, cross_entropy, cross_entropy_lambda, lambdarank, rank_xendcg, aliases: objective_type, app, application, loss\n",
    "\n",
    "  - eval_metric [ default according to objective ]\n",
    "    - The metric to be used for validation data.\n",
    "    - The default values are rmse for regression and error for classification.\n",
    "    - Typical values are:\n",
    "        -    rmse – root mean square error\n",
    "        -    mae – mean absolute error\n",
    "        -    logloss – negative log-likelihood\n",
    "        -    error – Binary classification error rate (0.5 threshold)\n",
    "        -    merror – Multiclass classification error rate\n",
    "        -    mlogloss – Multiclass logloss\n",
    "        -    auc: Area under the curve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595282de",
   "metadata": {},
   "source": [
    "[LightGBM]\n",
    "\n",
    "  - Hyperparameter tuning\n",
    "  - n_estimators, learning_rate, max_depth, reg_alpha\n",
    "  - LightGBM은 Hyperparam이 굉장히 많은 알고리즘 중에 하나임\n",
    "  - 위에 4가지만 잘 조정해도 좋은 결과를 얻을 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b68f3d31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> 0 <<<\n",
      "n_estimators : 5, learning_rate : 0.1, max_depth : 3, reg_alpha : 0.1\n",
      "[LightGBM] [Info] [cross_entropy:Init]: (objective) labels passed interval [0, 1] check\n",
      "[LightGBM] [Info] [cross_entropy:Init]: (metric) labels passed interval [0, 1] check\n",
      "[LightGBM] [Info] [cross_entropy:Init]: sum-of-weights = 372.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000880 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 905\n",
      "[LightGBM] [Info] Number of data points in the train set: 372, number of used features: 20\n",
      "[LightGBM] [Info] [cross_entropy:BoostFromScore]: pavg = 0.352151 -> initscore = -0.609600\n",
      "[LightGBM] [Info] Start training from score -0.609600\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Train Confusion Matrix\n",
      "[[237   4]\n",
      " [ 55  76]]\n",
      "Train Acc : 0.8413978494623656\n",
      "Train F1-Score : 0.7203791469194314\n",
      "Test Confusion Matrix\n",
      "[[103   7]\n",
      " [ 10  40]]\n",
      "TesT Acc : 0.89375\n",
      "Test F1-Score : 0.8247422680412372\n",
      "-----------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------\n",
      ">>> 1 <<<\n",
      "n_estimators : 5, learning_rate : 0.1, max_depth : 3, reg_alpha : 0.3\n",
      "[LightGBM] [Info] [cross_entropy:Init]: (objective) labels passed interval [0, 1] check\n",
      "[LightGBM] [Info] [cross_entropy:Init]: (metric) labels passed interval [0, 1] check\n",
      "[LightGBM] [Info] [cross_entropy:Init]: sum-of-weights = 372.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000172 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 905\n",
      "[LightGBM] [Info] Number of data points in the train set: 372, number of used features: 20\n",
      "[LightGBM] [Info] [cross_entropy:BoostFromScore]: pavg = 0.352151 -> initscore = -0.609600\n",
      "[LightGBM] [Info] Start training from score -0.609600\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Train Confusion Matrix\n",
      "[[237   4]\n",
      " [ 54  77]]\n",
      "Train Acc : 0.8440860215053764\n",
      "Train F1-Score : 0.7264150943396226\n",
      "Test Confusion Matrix\n",
      "[[102   8]\n",
      " [ 10  40]]\n",
      "TesT Acc : 0.8875\n",
      "Test F1-Score : 0.816326530612245\n",
      "-----------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------\n",
      ">>> 2 <<<\n",
      "n_estimators : 5, learning_rate : 0.1, max_depth : 3, reg_alpha : 0.5\n",
      "[LightGBM] [Info] [cross_entropy:Init]: (objective) labels passed interval [0, 1] check\n",
      "[LightGBM] [Info] [cross_entropy:Init]: (metric) labels passed interval [0, 1] check\n",
      "[LightGBM] [Info] [cross_entropy:Init]: sum-of-weights = 372.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000408 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 905\n",
      "[LightGBM] [Info] Number of data points in the train set: 372, number of used features: 20\n",
      "[LightGBM] [Info] [cross_entropy:BoostFromScore]: pavg = 0.352151 -> initscore = -0.609600\n",
      "[LightGBM] [Info] Start training from score -0.609600\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Train Confusion Matrix\n",
      "[[237   4]\n",
      " [ 54  77]]\n",
      "Train Acc : 0.8440860215053764\n",
      "Train F1-Score : 0.7264150943396226\n",
      "Test Confusion Matrix\n",
      "[[102   8]\n",
      " [ 10  40]]\n",
      "TesT Acc : 0.8875\n",
      "Test F1-Score : 0.816326530612245\n",
      "-----------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------\n",
      ">>> 3 <<<\n",
      "n_estimators : 5, learning_rate : 0.1, max_depth : 5, reg_alpha : 0.1\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] [cross_entropy:Init]: (objective) labels passed interval [0, 1] check\n",
      "[LightGBM] [Info] [cross_entropy:Init]: (metric) labels passed interval [0, 1] check\n",
      "[LightGBM] [Info] [cross_entropy:Init]: sum-of-weights = 372.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000243 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 905\n",
      "[LightGBM] [Info] Number of data points in the train set: 372, number of used features: 20\n",
      "[LightGBM] [Info] [cross_entropy:BoostFromScore]: pavg = 0.352151 -> initscore = -0.609600\n",
      "[LightGBM] [Info] Start training from score -0.609600\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "Train Confusion Matrix\n",
      "[[237   4]\n",
      " [ 44  87]]\n",
      "Train Acc : 0.8709677419354839\n",
      "Train F1-Score : 0.7837837837837838\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "Test Confusion Matrix\n",
      "[[101   9]\n",
      " [  9  41]]\n",
      "TesT Acc : 0.8875\n",
      "Test F1-Score : 0.82\n",
      "-----------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------\n",
      ">>> 4 <<<\n",
      "n_estimators : 5, learning_rate : 0.1, max_depth : 5, reg_alpha : 0.3\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] [cross_entropy:Init]: (objective) labels passed interval [0, 1] check\n",
      "[LightGBM] [Info] [cross_entropy:Init]: (metric) labels passed interval [0, 1] check\n",
      "[LightGBM] [Info] [cross_entropy:Init]: sum-of-weights = 372.000000\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000405 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 905\n",
      "[LightGBM] [Info] Number of data points in the train set: 372, number of used features: 20\n",
      "[LightGBM] [Info] [cross_entropy:BoostFromScore]: pavg = 0.352151 -> initscore = -0.609600\n",
      "[LightGBM] [Info] Start training from score -0.609600\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "Train Confusion Matrix\n",
      "[[236   5]\n",
      " [ 50  81]]\n",
      "Train Acc : 0.8521505376344086\n",
      "Train F1-Score : 0.7465437788018434\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "Test Confusion Matrix\n",
      "[[101   9]\n",
      " [ 10  40]]\n",
      "TesT Acc : 0.88125\n",
      "Test F1-Score : 0.8080808080808082\n",
      "-----------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------\n",
      ">>> 5 <<<\n",
      "n_estimators : 5, learning_rate : 0.1, max_depth : 5, reg_alpha : 0.5\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] [cross_entropy:Init]: (objective) labels passed interval [0, 1] check\n",
      "[LightGBM] [Info] [cross_entropy:Init]: (metric) labels passed interval [0, 1] check\n",
      "[LightGBM] [Info] [cross_entropy:Init]: sum-of-weights = 372.000000\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000255 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 905\n",
      "[LightGBM] [Info] Number of data points in the train set: 372, number of used features: 20\n",
      "[LightGBM] [Info] [cross_entropy:BoostFromScore]: pavg = 0.352151 -> initscore = -0.609600\n",
      "[LightGBM] [Info] Start training from score -0.609600\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "Train Confusion Matrix\n",
      "[[236   5]\n",
      " [ 44  87]]\n",
      "Train Acc : 0.8682795698924731\n",
      "Train F1-Score : 0.7802690582959643\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "Test Confusion Matrix\n",
      "[[101   9]\n",
      " [  9  41]]\n",
      "TesT Acc : 0.8875\n",
      "Test F1-Score : 0.82\n",
      "-----------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------\n",
      ">>> 6 <<<\n",
      "n_estimators : 5, learning_rate : 0.3, max_depth : 3, reg_alpha : 0.1\n",
      "[LightGBM] [Info] [cross_entropy:Init]: (objective) labels passed interval [0, 1] check\n",
      "[LightGBM] [Info] [cross_entropy:Init]: (metric) labels passed interval [0, 1] check\n",
      "[LightGBM] [Info] [cross_entropy:Init]: sum-of-weights = 372.000000\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000368 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 905\n",
      "[LightGBM] [Info] Number of data points in the train set: 372, number of used features: 20\n",
      "[LightGBM] [Info] [cross_entropy:BoostFromScore]: pavg = 0.352151 -> initscore = -0.609600\n",
      "[LightGBM] [Info] Start training from score -0.609600\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Train Confusion Matrix\n",
      "[[227  14]\n",
      " [ 17 114]]\n",
      "Train Acc : 0.9166666666666666\n",
      "Train F1-Score : 0.8803088803088803\n",
      "Test Confusion Matrix\n",
      "[[95 15]\n",
      " [ 1 49]]\n",
      "TesT Acc : 0.9\n",
      "Test F1-Score : 0.8596491228070174\n",
      "-----------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------\n",
      ">>> 7 <<<\n",
      "n_estimators : 5, learning_rate : 0.3, max_depth : 3, reg_alpha : 0.3\n",
      "[LightGBM] [Info] [cross_entropy:Init]: (objective) labels passed interval [0, 1] check\n",
      "[LightGBM] [Info] [cross_entropy:Init]: (metric) labels passed interval [0, 1] check\n",
      "[LightGBM] [Info] [cross_entropy:Init]: sum-of-weights = 372.000000\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000305 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 905\n",
      "[LightGBM] [Info] Number of data points in the train set: 372, number of used features: 20\n",
      "[LightGBM] [Info] [cross_entropy:BoostFromScore]: pavg = 0.352151 -> initscore = -0.609600\n",
      "[LightGBM] [Info] Start training from score -0.609600\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Train Confusion Matrix\n",
      "[[228  13]\n",
      " [ 19 112]]\n",
      "Train Acc : 0.9139784946236559\n",
      "Train F1-Score : 0.875\n",
      "Test Confusion Matrix\n",
      "[[96 14]\n",
      " [ 1 49]]\n",
      "TesT Acc : 0.90625\n",
      "Test F1-Score : 0.8672566371681417\n",
      "-----------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------\n",
      ">>> 8 <<<\n",
      "n_estimators : 5, learning_rate : 0.3, max_depth : 3, reg_alpha : 0.5\n",
      "[LightGBM] [Info] [cross_entropy:Init]: (objective) labels passed interval [0, 1] check\n",
      "[LightGBM] [Info] [cross_entropy:Init]: (metric) labels passed interval [0, 1] check\n",
      "[LightGBM] [Info] [cross_entropy:Init]: sum-of-weights = 372.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000142 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 905\n",
      "[LightGBM] [Info] Number of data points in the train set: 372, number of used features: 20\n",
      "[LightGBM] [Info] [cross_entropy:BoostFromScore]: pavg = 0.352151 -> initscore = -0.609600\n",
      "[LightGBM] [Info] Start training from score -0.609600\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Train Confusion Matrix\n",
      "[[228  13]\n",
      " [ 17 114]]\n",
      "Train Acc : 0.9193548387096774\n",
      "Train F1-Score : 0.883720930232558\n",
      "Test Confusion Matrix\n",
      "[[96 14]\n",
      " [ 1 49]]\n",
      "TesT Acc : 0.90625\n",
      "Test F1-Score : 0.8672566371681417\n",
      "-----------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------\n",
      ">>> 9 <<<\n",
      "n_estimators : 5, learning_rate : 0.3, max_depth : 5, reg_alpha : 0.1\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] [cross_entropy:Init]: (objective) labels passed interval [0, 1] check\n",
      "[LightGBM] [Info] [cross_entropy:Init]: (metric) labels passed interval [0, 1] check\n",
      "[LightGBM] [Info] [cross_entropy:Init]: sum-of-weights = 372.000000\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000457 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 905\n",
      "[LightGBM] [Info] Number of data points in the train set: 372, number of used features: 20\n",
      "[LightGBM] [Info] [cross_entropy:BoostFromScore]: pavg = 0.352151 -> initscore = -0.609600\n",
      "[LightGBM] [Info] Start training from score -0.609600\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "Train Confusion Matrix\n",
      "[[232   9]\n",
      " [ 20 111]]\n",
      "Train Acc : 0.9220430107526881\n",
      "Train F1-Score : 0.8844621513944223\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "Test Confusion Matrix\n",
      "[[95 15]\n",
      " [ 4 46]]\n",
      "TesT Acc : 0.88125\n",
      "Test F1-Score : 0.8288288288288288\n",
      "-----------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------\n",
      ">>> 10 <<<\n",
      "n_estimators : 5, learning_rate : 0.3, max_depth : 5, reg_alpha : 0.3\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] [cross_entropy:Init]: (objective) labels passed interval [0, 1] check\n",
      "[LightGBM] [Info] [cross_entropy:Init]: (metric) labels passed interval [0, 1] check\n",
      "[LightGBM] [Info] [cross_entropy:Init]: sum-of-weights = 372.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000131 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 905\n",
      "[LightGBM] [Info] Number of data points in the train set: 372, number of used features: 20\n",
      "[LightGBM] [Info] [cross_entropy:BoostFromScore]: pavg = 0.352151 -> initscore = -0.609600\n",
      "[LightGBM] [Info] Start training from score -0.609600\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "Train Confusion Matrix\n",
      "[[232   9]\n",
      " [ 17 114]]\n",
      "Train Acc : 0.9301075268817204\n",
      "Train F1-Score : 0.8976377952755905\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Confusion Matrix\n",
      "[[94 16]\n",
      " [ 5 45]]\n",
      "TesT Acc : 0.86875\n",
      "Test F1-Score : 0.8108108108108109\n",
      "-----------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------\n",
      ">>> 11 <<<\n",
      "n_estimators : 5, learning_rate : 0.3, max_depth : 5, reg_alpha : 0.5\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] [cross_entropy:Init]: (objective) labels passed interval [0, 1] check\n",
      "[LightGBM] [Info] [cross_entropy:Init]: (metric) labels passed interval [0, 1] check\n",
      "[LightGBM] [Info] [cross_entropy:Init]: sum-of-weights = 372.000000\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000342 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 905\n",
      "[LightGBM] [Info] Number of data points in the train set: 372, number of used features: 20\n",
      "[LightGBM] [Info] [cross_entropy:BoostFromScore]: pavg = 0.352151 -> initscore = -0.609600\n",
      "[LightGBM] [Info] Start training from score -0.609600\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "Train Confusion Matrix\n",
      "[[231  10]\n",
      " [ 21 110]]\n",
      "Train Acc : 0.9166666666666666\n",
      "Train F1-Score : 0.8764940239043825\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "Test Confusion Matrix\n",
      "[[97 13]\n",
      " [ 3 47]]\n",
      "TesT Acc : 0.9\n",
      "Test F1-Score : 0.8545454545454546\n",
      "-----------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------\n",
      ">>> 12 <<<\n",
      "n_estimators : 10, learning_rate : 0.1, max_depth : 3, reg_alpha : 0.1\n",
      "[LightGBM] [Info] [cross_entropy:Init]: (objective) labels passed interval [0, 1] check\n",
      "[LightGBM] [Info] [cross_entropy:Init]: (metric) labels passed interval [0, 1] check\n",
      "[LightGBM] [Info] [cross_entropy:Init]: sum-of-weights = 372.000000\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000597 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 905\n",
      "[LightGBM] [Info] Number of data points in the train set: 372, number of used features: 20\n",
      "[LightGBM] [Info] [cross_entropy:BoostFromScore]: pavg = 0.352151 -> initscore = -0.609600\n",
      "[LightGBM] [Info] Start training from score -0.609600\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Train Confusion Matrix\n",
      "[[233   8]\n",
      " [ 36  95]]\n",
      "Train Acc : 0.8817204301075269\n",
      "Train F1-Score : 0.811965811965812\n",
      "Test Confusion Matrix\n",
      "[[98 12]\n",
      " [ 4 46]]\n",
      "TesT Acc : 0.9\n",
      "Test F1-Score : 0.851851851851852\n",
      "-----------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------\n",
      ">>> 13 <<<\n",
      "n_estimators : 10, learning_rate : 0.1, max_depth : 3, reg_alpha : 0.3\n",
      "[LightGBM] [Info] [cross_entropy:Init]: (objective) labels passed interval [0, 1] check\n",
      "[LightGBM] [Info] [cross_entropy:Init]: (metric) labels passed interval [0, 1] check\n",
      "[LightGBM] [Info] [cross_entropy:Init]: sum-of-weights = 372.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000338 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 905\n",
      "[LightGBM] [Info] Number of data points in the train set: 372, number of used features: 20\n",
      "[LightGBM] [Info] [cross_entropy:BoostFromScore]: pavg = 0.352151 -> initscore = -0.609600\n",
      "[LightGBM] [Info] Start training from score -0.609600\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Train Confusion Matrix\n",
      "[[233   8]\n",
      " [ 36  95]]\n",
      "Train Acc : 0.8817204301075269\n",
      "Train F1-Score : 0.811965811965812\n",
      "Test Confusion Matrix\n",
      "[[98 12]\n",
      " [ 4 46]]\n",
      "TesT Acc : 0.9\n",
      "Test F1-Score : 0.851851851851852\n",
      "-----------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------\n",
      ">>> 14 <<<\n",
      "n_estimators : 10, learning_rate : 0.1, max_depth : 3, reg_alpha : 0.5\n",
      "[LightGBM] [Info] [cross_entropy:Init]: (objective) labels passed interval [0, 1] check\n",
      "[LightGBM] [Info] [cross_entropy:Init]: (metric) labels passed interval [0, 1] check\n",
      "[LightGBM] [Info] [cross_entropy:Init]: sum-of-weights = 372.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000395 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 905\n",
      "[LightGBM] [Info] Number of data points in the train set: 372, number of used features: 20\n",
      "[LightGBM] [Info] [cross_entropy:BoostFromScore]: pavg = 0.352151 -> initscore = -0.609600\n",
      "[LightGBM] [Info] Start training from score -0.609600\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Train Confusion Matrix\n",
      "[[233   8]\n",
      " [ 33  98]]\n",
      "Train Acc : 0.8897849462365591\n",
      "Train F1-Score : 0.8270042194092826\n",
      "Test Confusion Matrix\n",
      "[[98 12]\n",
      " [ 5 45]]\n",
      "TesT Acc : 0.89375\n",
      "Test F1-Score : 0.8411214953271027\n",
      "-----------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------\n",
      ">>> 15 <<<\n",
      "n_estimators : 10, learning_rate : 0.1, max_depth : 5, reg_alpha : 0.1\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] [cross_entropy:Init]: (objective) labels passed interval [0, 1] check\n",
      "[LightGBM] [Info] [cross_entropy:Init]: (metric) labels passed interval [0, 1] check\n",
      "[LightGBM] [Info] [cross_entropy:Init]: sum-of-weights = 372.000000\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000359 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 905\n",
      "[LightGBM] [Info] Number of data points in the train set: 372, number of used features: 20\n",
      "[LightGBM] [Info] [cross_entropy:BoostFromScore]: pavg = 0.352151 -> initscore = -0.609600\n",
      "[LightGBM] [Info] Start training from score -0.609600\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "Train Confusion Matrix\n",
      "[[232   9]\n",
      " [ 28 103]]\n",
      "Train Acc : 0.9005376344086021\n",
      "Train F1-Score : 0.8477366255144032\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "Test Confusion Matrix\n",
      "[[97 13]\n",
      " [ 4 46]]\n",
      "TesT Acc : 0.89375\n",
      "Test F1-Score : 0.8440366972477064\n",
      "-----------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------\n",
      ">>> 16 <<<\n",
      "n_estimators : 10, learning_rate : 0.1, max_depth : 5, reg_alpha : 0.3\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] [cross_entropy:Init]: (objective) labels passed interval [0, 1] check\n",
      "[LightGBM] [Info] [cross_entropy:Init]: (metric) labels passed interval [0, 1] check\n",
      "[LightGBM] [Info] [cross_entropy:Init]: sum-of-weights = 372.000000\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000300 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 905\n",
      "[LightGBM] [Info] Number of data points in the train set: 372, number of used features: 20\n",
      "[LightGBM] [Info] [cross_entropy:BoostFromScore]: pavg = 0.352151 -> initscore = -0.609600\n",
      "[LightGBM] [Info] Start training from score -0.609600\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "Train Confusion Matrix\n",
      "[[233   8]\n",
      " [ 25 106]]\n",
      "Train Acc : 0.9112903225806451\n",
      "Train F1-Score : 0.8653061224489796\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "Test Confusion Matrix\n",
      "[[96 14]\n",
      " [ 4 46]]\n",
      "TesT Acc : 0.8875\n",
      "Test F1-Score : 0.8363636363636363\n",
      "-----------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------\n",
      ">>> 17 <<<\n",
      "n_estimators : 10, learning_rate : 0.1, max_depth : 5, reg_alpha : 0.5\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] [cross_entropy:Init]: (objective) labels passed interval [0, 1] check\n",
      "[LightGBM] [Info] [cross_entropy:Init]: (metric) labels passed interval [0, 1] check\n",
      "[LightGBM] [Info] [cross_entropy:Init]: sum-of-weights = 372.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000335 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 905\n",
      "[LightGBM] [Info] Number of data points in the train set: 372, number of used features: 20\n",
      "[LightGBM] [Info] [cross_entropy:BoostFromScore]: pavg = 0.352151 -> initscore = -0.609600\n",
      "[LightGBM] [Info] Start training from score -0.609600\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "Train Confusion Matrix\n",
      "[[232   9]\n",
      " [ 28 103]]\n",
      "Train Acc : 0.9005376344086021\n",
      "Train F1-Score : 0.8477366255144032\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "Test Confusion Matrix\n",
      "[[97 13]\n",
      " [ 4 46]]\n",
      "TesT Acc : 0.89375\n",
      "Test F1-Score : 0.8440366972477064\n",
      "-----------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------\n",
      ">>> 18 <<<\n",
      "n_estimators : 10, learning_rate : 0.3, max_depth : 3, reg_alpha : 0.1\n",
      "[LightGBM] [Info] [cross_entropy:Init]: (objective) labels passed interval [0, 1] check\n",
      "[LightGBM] [Info] [cross_entropy:Init]: (metric) labels passed interval [0, 1] check\n",
      "[LightGBM] [Info] [cross_entropy:Init]: sum-of-weights = 372.000000\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000358 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 905\n",
      "[LightGBM] [Info] Number of data points in the train set: 372, number of used features: 20\n",
      "[LightGBM] [Info] [cross_entropy:BoostFromScore]: pavg = 0.352151 -> initscore = -0.609600\n",
      "[LightGBM] [Info] Start training from score -0.609600\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Train Confusion Matrix\n",
      "[[227  14]\n",
      " [ 12 119]]\n",
      "Train Acc : 0.9301075268817204\n",
      "Train F1-Score : 0.9015151515151515\n",
      "Test Confusion Matrix\n",
      "[[92 18]\n",
      " [ 1 49]]\n",
      "TesT Acc : 0.88125\n",
      "Test F1-Score : 0.8376068376068375\n",
      "-----------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------\n",
      ">>> 19 <<<\n",
      "n_estimators : 10, learning_rate : 0.3, max_depth : 3, reg_alpha : 0.3\n",
      "[LightGBM] [Info] [cross_entropy:Init]: (objective) labels passed interval [0, 1] check\n",
      "[LightGBM] [Info] [cross_entropy:Init]: (metric) labels passed interval [0, 1] check\n",
      "[LightGBM] [Info] [cross_entropy:Init]: sum-of-weights = 372.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012701 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 905\n",
      "[LightGBM] [Info] Number of data points in the train set: 372, number of used features: 20\n",
      "[LightGBM] [Info] [cross_entropy:BoostFromScore]: pavg = 0.352151 -> initscore = -0.609600\n",
      "[LightGBM] [Info] Start training from score -0.609600\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Train Confusion Matrix\n",
      "[[229  12]\n",
      " [ 10 121]]\n",
      "Train Acc : 0.9408602150537635\n",
      "Train F1-Score : 0.9166666666666667\n",
      "Test Confusion Matrix\n",
      "[[92 18]\n",
      " [ 1 49]]\n",
      "TesT Acc : 0.88125\n",
      "Test F1-Score : 0.8376068376068375\n",
      "-----------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------\n",
      ">>> 20 <<<\n",
      "n_estimators : 10, learning_rate : 0.3, max_depth : 3, reg_alpha : 0.5\n",
      "[LightGBM] [Info] [cross_entropy:Init]: (objective) labels passed interval [0, 1] check\n",
      "[LightGBM] [Info] [cross_entropy:Init]: (metric) labels passed interval [0, 1] check\n",
      "[LightGBM] [Info] [cross_entropy:Init]: sum-of-weights = 372.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000203 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 905\n",
      "[LightGBM] [Info] Number of data points in the train set: 372, number of used features: 20\n",
      "[LightGBM] [Info] [cross_entropy:BoostFromScore]: pavg = 0.352151 -> initscore = -0.609600\n",
      "[LightGBM] [Info] Start training from score -0.609600\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Train Confusion Matrix\n",
      "[[229  12]\n",
      " [ 13 118]]\n",
      "Train Acc : 0.9327956989247311\n",
      "Train F1-Score : 0.9042145593869731\n",
      "Test Confusion Matrix\n",
      "[[92 18]\n",
      " [ 1 49]]\n",
      "TesT Acc : 0.88125\n",
      "Test F1-Score : 0.8376068376068375\n",
      "-----------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------\n",
      ">>> 21 <<<\n",
      "n_estimators : 10, learning_rate : 0.3, max_depth : 5, reg_alpha : 0.1\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] [cross_entropy:Init]: (objective) labels passed interval [0, 1] check\n",
      "[LightGBM] [Info] [cross_entropy:Init]: (metric) labels passed interval [0, 1] check\n",
      "[LightGBM] [Info] [cross_entropy:Init]: sum-of-weights = 372.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000268 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 905\n",
      "[LightGBM] [Info] Number of data points in the train set: 372, number of used features: 20\n",
      "[LightGBM] [Info] [cross_entropy:BoostFromScore]: pavg = 0.352151 -> initscore = -0.609600\n",
      "[LightGBM] [Info] Start training from score -0.609600\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "Train Confusion Matrix\n",
      "[[233   8]\n",
      " [ 12 119]]\n",
      "Train Acc : 0.946236559139785\n",
      "Train F1-Score : 0.9224806201550387\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "Test Confusion Matrix\n",
      "[[98 12]\n",
      " [ 3 47]]\n",
      "TesT Acc : 0.90625\n",
      "Test F1-Score : 0.8623853211009174\n",
      "-----------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------\n",
      ">>> 22 <<<\n",
      "n_estimators : 10, learning_rate : 0.3, max_depth : 5, reg_alpha : 0.3\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] [cross_entropy:Init]: (objective) labels passed interval [0, 1] check\n",
      "[LightGBM] [Info] [cross_entropy:Init]: (metric) labels passed interval [0, 1] check\n",
      "[LightGBM] [Info] [cross_entropy:Init]: sum-of-weights = 372.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000385 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 905\n",
      "[LightGBM] [Info] Number of data points in the train set: 372, number of used features: 20\n",
      "[LightGBM] [Info] [cross_entropy:BoostFromScore]: pavg = 0.352151 -> initscore = -0.609600\n",
      "[LightGBM] [Info] Start training from score -0.609600\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "Train Confusion Matrix\n",
      "[[234   7]\n",
      " [ 10 121]]\n",
      "Train Acc : 0.9543010752688172\n",
      "Train F1-Score : 0.9343629343629344\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "Test Confusion Matrix\n",
      "[[96 14]\n",
      " [ 4 46]]\n",
      "TesT Acc : 0.8875\n",
      "Test F1-Score : 0.8363636363636363\n",
      "-----------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------\n",
      ">>> 23 <<<\n",
      "n_estimators : 10, learning_rate : 0.3, max_depth : 5, reg_alpha : 0.5\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] [cross_entropy:Init]: (objective) labels passed interval [0, 1] check\n",
      "[LightGBM] [Info] [cross_entropy:Init]: (metric) labels passed interval [0, 1] check\n",
      "[LightGBM] [Info] [cross_entropy:Init]: sum-of-weights = 372.000000\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000181 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 905\n",
      "[LightGBM] [Info] Number of data points in the train set: 372, number of used features: 20\n",
      "[LightGBM] [Info] [cross_entropy:BoostFromScore]: pavg = 0.352151 -> initscore = -0.609600\n",
      "[LightGBM] [Info] Start training from score -0.609600\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "Train Confusion Matrix\n",
      "[[234   7]\n",
      " [ 11 120]]\n",
      "Train Acc : 0.9516129032258065\n",
      "Train F1-Score : 0.9302325581395349\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "Test Confusion Matrix\n",
      "[[96 14]\n",
      " [ 4 46]]\n",
      "TesT Acc : 0.8875\n",
      "Test F1-Score : 0.8363636363636363\n",
      "-----------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------\n",
      ">>> 24 <<<\n",
      "n_estimators : 20, learning_rate : 0.1, max_depth : 3, reg_alpha : 0.1\n",
      "[LightGBM] [Info] [cross_entropy:Init]: (objective) labels passed interval [0, 1] check\n",
      "[LightGBM] [Info] [cross_entropy:Init]: (metric) labels passed interval [0, 1] check\n",
      "[LightGBM] [Info] [cross_entropy:Init]: sum-of-weights = 372.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000244 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 905\n",
      "[LightGBM] [Info] Number of data points in the train set: 372, number of used features: 20\n",
      "[LightGBM] [Info] [cross_entropy:BoostFromScore]: pavg = 0.352151 -> initscore = -0.609600\n",
      "[LightGBM] [Info] Start training from score -0.609600\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Train Confusion Matrix\n",
      "[[229  12]\n",
      " [ 17 114]]\n",
      "Train Acc : 0.9220430107526881\n",
      "Train F1-Score : 0.8871595330739299\n",
      "Test Confusion Matrix\n",
      "[[95 15]\n",
      " [ 2 48]]\n",
      "TesT Acc : 0.89375\n",
      "Test F1-Score : 0.8495575221238937\n",
      "-----------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------\n",
      ">>> 25 <<<\n",
      "n_estimators : 20, learning_rate : 0.1, max_depth : 3, reg_alpha : 0.3\n",
      "[LightGBM] [Info] [cross_entropy:Init]: (objective) labels passed interval [0, 1] check\n",
      "[LightGBM] [Info] [cross_entropy:Init]: (metric) labels passed interval [0, 1] check\n",
      "[LightGBM] [Info] [cross_entropy:Init]: sum-of-weights = 372.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000276 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 905\n",
      "[LightGBM] [Info] Number of data points in the train set: 372, number of used features: 20\n",
      "[LightGBM] [Info] [cross_entropy:BoostFromScore]: pavg = 0.352151 -> initscore = -0.609600\n",
      "[LightGBM] [Info] Start training from score -0.609600\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Train Confusion Matrix\n",
      "[[229  12]\n",
      " [ 14 117]]\n",
      "Train Acc : 0.9301075268817204\n",
      "Train F1-Score : 0.9\n",
      "Test Confusion Matrix\n",
      "[[94 16]\n",
      " [ 1 49]]\n",
      "TesT Acc : 0.89375\n",
      "Test F1-Score : 0.8521739130434782\n",
      "-----------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------\n",
      ">>> 26 <<<\n",
      "n_estimators : 20, learning_rate : 0.1, max_depth : 3, reg_alpha : 0.5\n",
      "[LightGBM] [Info] [cross_entropy:Init]: (objective) labels passed interval [0, 1] check\n",
      "[LightGBM] [Info] [cross_entropy:Init]: (metric) labels passed interval [0, 1] check\n",
      "[LightGBM] [Info] [cross_entropy:Init]: sum-of-weights = 372.000000\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000217 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 905\n",
      "[LightGBM] [Info] Number of data points in the train set: 372, number of used features: 20\n",
      "[LightGBM] [Info] [cross_entropy:BoostFromScore]: pavg = 0.352151 -> initscore = -0.609600\n",
      "[LightGBM] [Info] Start training from score -0.609600\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Train Confusion Matrix\n",
      "[[228  13]\n",
      " [ 13 118]]\n",
      "Train Acc : 0.9301075268817204\n",
      "Train F1-Score : 0.9007633587786259\n",
      "Test Confusion Matrix\n",
      "[[94 16]\n",
      " [ 1 49]]\n",
      "TesT Acc : 0.89375\n",
      "Test F1-Score : 0.8521739130434782\n",
      "-----------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------\n",
      ">>> 27 <<<\n",
      "n_estimators : 20, learning_rate : 0.1, max_depth : 5, reg_alpha : 0.1\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] [cross_entropy:Init]: (objective) labels passed interval [0, 1] check\n",
      "[LightGBM] [Info] [cross_entropy:Init]: (metric) labels passed interval [0, 1] check\n",
      "[LightGBM] [Info] [cross_entropy:Init]: sum-of-weights = 372.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000336 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 905\n",
      "[LightGBM] [Info] Number of data points in the train set: 372, number of used features: 20\n",
      "[LightGBM] [Info] [cross_entropy:BoostFromScore]: pavg = 0.352151 -> initscore = -0.609600\n",
      "[LightGBM] [Info] Start training from score -0.609600\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "Train Confusion Matrix\n",
      "[[233   8]\n",
      " [ 15 116]]\n",
      "Train Acc : 0.9381720430107527\n",
      "Train F1-Score : 0.9098039215686274\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "Test Confusion Matrix\n",
      "[[95 15]\n",
      " [ 3 47]]\n",
      "TesT Acc : 0.8875\n",
      "Test F1-Score : 0.8392857142857143\n",
      "-----------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------\n",
      ">>> 28 <<<\n",
      "n_estimators : 20, learning_rate : 0.1, max_depth : 5, reg_alpha : 0.3\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] [cross_entropy:Init]: (objective) labels passed interval [0, 1] check\n",
      "[LightGBM] [Info] [cross_entropy:Init]: (metric) labels passed interval [0, 1] check\n",
      "[LightGBM] [Info] [cross_entropy:Init]: sum-of-weights = 372.000000\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000325 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 905\n",
      "[LightGBM] [Info] Number of data points in the train set: 372, number of used features: 20\n",
      "[LightGBM] [Info] [cross_entropy:BoostFromScore]: pavg = 0.352151 -> initscore = -0.609600\n",
      "[LightGBM] [Info] Start training from score -0.609600\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "Train Confusion Matrix\n",
      "[[232   9]\n",
      " [ 14 117]]\n",
      "Train Acc : 0.9381720430107527\n",
      "Train F1-Score : 0.9105058365758756\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "Test Confusion Matrix\n",
      "[[95 15]\n",
      " [ 2 48]]\n",
      "TesT Acc : 0.89375\n",
      "Test F1-Score : 0.8495575221238937\n",
      "-----------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------\n",
      ">>> 29 <<<\n",
      "n_estimators : 20, learning_rate : 0.1, max_depth : 5, reg_alpha : 0.5\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] [cross_entropy:Init]: (objective) labels passed interval [0, 1] check\n",
      "[LightGBM] [Info] [cross_entropy:Init]: (metric) labels passed interval [0, 1] check\n",
      "[LightGBM] [Info] [cross_entropy:Init]: sum-of-weights = 372.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000365 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 905\n",
      "[LightGBM] [Info] Number of data points in the train set: 372, number of used features: 20\n",
      "[LightGBM] [Info] [cross_entropy:BoostFromScore]: pavg = 0.352151 -> initscore = -0.609600\n",
      "[LightGBM] [Info] Start training from score -0.609600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "Train Confusion Matrix\n",
      "[[232   9]\n",
      " [ 17 114]]\n",
      "Train Acc : 0.9301075268817204\n",
      "Train F1-Score : 0.8976377952755905\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "Test Confusion Matrix\n",
      "[[96 14]\n",
      " [ 3 47]]\n",
      "TesT Acc : 0.89375\n",
      "Test F1-Score : 0.8468468468468469\n",
      "-----------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------\n",
      ">>> 30 <<<\n",
      "n_estimators : 20, learning_rate : 0.3, max_depth : 3, reg_alpha : 0.1\n",
      "[LightGBM] [Info] [cross_entropy:Init]: (objective) labels passed interval [0, 1] check\n",
      "[LightGBM] [Info] [cross_entropy:Init]: (metric) labels passed interval [0, 1] check\n",
      "[LightGBM] [Info] [cross_entropy:Init]: sum-of-weights = 372.000000\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000396 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 905\n",
      "[LightGBM] [Info] Number of data points in the train set: 372, number of used features: 20\n",
      "[LightGBM] [Info] [cross_entropy:BoostFromScore]: pavg = 0.352151 -> initscore = -0.609600\n",
      "[LightGBM] [Info] Start training from score -0.609600\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Train Confusion Matrix\n",
      "[[236   5]\n",
      " [  6 125]]\n",
      "Train Acc : 0.9704301075268817\n",
      "Train F1-Score : 0.9578544061302683\n",
      "Test Confusion Matrix\n",
      "[[89 21]\n",
      " [ 1 49]]\n",
      "TesT Acc : 0.8625\n",
      "Test F1-Score : 0.8166666666666667\n",
      "-----------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------\n",
      ">>> 31 <<<\n",
      "n_estimators : 20, learning_rate : 0.3, max_depth : 3, reg_alpha : 0.3\n",
      "[LightGBM] [Info] [cross_entropy:Init]: (objective) labels passed interval [0, 1] check\n",
      "[LightGBM] [Info] [cross_entropy:Init]: (metric) labels passed interval [0, 1] check\n",
      "[LightGBM] [Info] [cross_entropy:Init]: sum-of-weights = 372.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000140 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 905\n",
      "[LightGBM] [Info] Number of data points in the train set: 372, number of used features: 20\n",
      "[LightGBM] [Info] [cross_entropy:BoostFromScore]: pavg = 0.352151 -> initscore = -0.609600\n",
      "[LightGBM] [Info] Start training from score -0.609600\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Train Confusion Matrix\n",
      "[[233   8]\n",
      " [  6 125]]\n",
      "Train Acc : 0.9623655913978495\n",
      "Train F1-Score : 0.9469696969696969\n",
      "Test Confusion Matrix\n",
      "[[91 19]\n",
      " [ 1 49]]\n",
      "TesT Acc : 0.875\n",
      "Test F1-Score : 0.8305084745762712\n",
      "-----------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------\n",
      ">>> 32 <<<\n",
      "n_estimators : 20, learning_rate : 0.3, max_depth : 3, reg_alpha : 0.5\n",
      "[LightGBM] [Info] [cross_entropy:Init]: (objective) labels passed interval [0, 1] check\n",
      "[LightGBM] [Info] [cross_entropy:Init]: (metric) labels passed interval [0, 1] check\n",
      "[LightGBM] [Info] [cross_entropy:Init]: sum-of-weights = 372.000000\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000354 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 905\n",
      "[LightGBM] [Info] Number of data points in the train set: 372, number of used features: 20\n",
      "[LightGBM] [Info] [cross_entropy:BoostFromScore]: pavg = 0.352151 -> initscore = -0.609600\n",
      "[LightGBM] [Info] Start training from score -0.609600\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Train Confusion Matrix\n",
      "[[234   7]\n",
      " [  8 123]]\n",
      "Train Acc : 0.9596774193548387\n",
      "Train F1-Score : 0.9425287356321839\n",
      "Test Confusion Matrix\n",
      "[[93 17]\n",
      " [ 1 49]]\n",
      "TesT Acc : 0.8875\n",
      "Test F1-Score : 0.8448275862068965\n",
      "-----------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------\n",
      ">>> 33 <<<\n",
      "n_estimators : 20, learning_rate : 0.3, max_depth : 5, reg_alpha : 0.1\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] [cross_entropy:Init]: (objective) labels passed interval [0, 1] check\n",
      "[LightGBM] [Info] [cross_entropy:Init]: (metric) labels passed interval [0, 1] check\n",
      "[LightGBM] [Info] [cross_entropy:Init]: sum-of-weights = 372.000000\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000291 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 905\n",
      "[LightGBM] [Info] Number of data points in the train set: 372, number of used features: 20\n",
      "[LightGBM] [Info] [cross_entropy:BoostFromScore]: pavg = 0.352151 -> initscore = -0.609600\n",
      "[LightGBM] [Info] Start training from score -0.609600\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "Train Confusion Matrix\n",
      "[[239   2]\n",
      " [  2 129]]\n",
      "Train Acc : 0.989247311827957\n",
      "Train F1-Score : 0.9847328244274809\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "Test Confusion Matrix\n",
      "[[94 16]\n",
      " [ 2 48]]\n",
      "TesT Acc : 0.8875\n",
      "Test F1-Score : 0.8421052631578947\n",
      "-----------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------\n",
      ">>> 34 <<<\n",
      "n_estimators : 20, learning_rate : 0.3, max_depth : 5, reg_alpha : 0.3\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] [cross_entropy:Init]: (objective) labels passed interval [0, 1] check\n",
      "[LightGBM] [Info] [cross_entropy:Init]: (metric) labels passed interval [0, 1] check\n",
      "[LightGBM] [Info] [cross_entropy:Init]: sum-of-weights = 372.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000435 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 905\n",
      "[LightGBM] [Info] Number of data points in the train set: 372, number of used features: 20\n",
      "[LightGBM] [Info] [cross_entropy:BoostFromScore]: pavg = 0.352151 -> initscore = -0.609600\n",
      "[LightGBM] [Info] Start training from score -0.609600\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "Train Confusion Matrix\n",
      "[[237   4]\n",
      " [  0 131]]\n",
      "Train Acc : 0.989247311827957\n",
      "Train F1-Score : 0.9849624060150376\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "Test Confusion Matrix\n",
      "[[93 17]\n",
      " [ 2 48]]\n",
      "TesT Acc : 0.88125\n",
      "Test F1-Score : 0.8347826086956522\n",
      "-----------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------\n",
      ">>> 35 <<<\n",
      "n_estimators : 20, learning_rate : 0.3, max_depth : 5, reg_alpha : 0.5\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] [cross_entropy:Init]: (objective) labels passed interval [0, 1] check\n",
      "[LightGBM] [Info] [cross_entropy:Init]: (metric) labels passed interval [0, 1] check\n",
      "[LightGBM] [Info] [cross_entropy:Init]: sum-of-weights = 372.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000304 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 905\n",
      "[LightGBM] [Info] Number of data points in the train set: 372, number of used features: 20\n",
      "[LightGBM] [Info] [cross_entropy:BoostFromScore]: pavg = 0.352151 -> initscore = -0.609600\n",
      "[LightGBM] [Info] Start training from score -0.609600\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "Train Confusion Matrix\n",
      "[[237   4]\n",
      " [  1 130]]\n",
      "Train Acc : 0.9865591397849462\n",
      "Train F1-Score : 0.9811320754716981\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "Test Confusion Matrix\n",
      "[[92 18]\n",
      " [ 4 46]]\n",
      "TesT Acc : 0.8625\n",
      "Test F1-Score : 0.8070175438596492\n",
      "-----------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# n_estimators\n",
    "n_tree = [5, 10, 20]\n",
    "# learning_rate\n",
    "l_rate = [0.1, 0.3]\n",
    "# max_depth\n",
    "m_depth = [3, 5]\n",
    "# reg_alpha\n",
    "L1_norm = [0.1, 0.3, 0.5]\n",
    "\n",
    "# Modeling\n",
    "save_n = []\n",
    "save_l = []\n",
    "save_m = []\n",
    "save_L1 = []\n",
    "f1_score_ = []\n",
    "\n",
    "cnt = 0\n",
    "\n",
    "for n in n_tree:\n",
    "    for l in l_rate:\n",
    "        for m in m_depth:\n",
    "            for L1 in L1_norm:\n",
    "                \n",
    "                print(\">>> {} <<<\".format(cnt))\n",
    "                cnt +=1\n",
    "                print(\"n_estimators : {}, learning_rate : {}, max_depth : {}, reg_alpha : {}\".format(n, l, m, L1))\n",
    "                model = LGBMClassifier(n_estimators=n, learning_rate=l, \n",
    "                                       max_depth=m, reg_alpha=L1, \n",
    "                                       n_jobs=-1, objective='cross_entropy')\n",
    "                model.fit(X.iloc[train_idx], Y.iloc[train_idx])\n",
    "                \n",
    "                \n",
    "                # Train Acc\n",
    "                y_pre_train = model.predict(X.iloc[train_idx])\n",
    "                cm_train = confusion_matrix(Y.iloc[train_idx], y_pre_train)\n",
    "                print(\"Train Confusion Matrix\")\n",
    "                print(cm_train)\n",
    "                print(\"Train Acc : {}\".format((cm_train[0,0] + cm_train[1,1])/cm_train.sum()))\n",
    "                print(\"Train F1-Score : {}\".format(f1_score(Y.iloc[train_idx], y_pre_train)))\n",
    "\n",
    "                # Test Acc\n",
    "                y_pre_test = model.predict(X.iloc[valid_idx])\n",
    "                cm_test = confusion_matrix(Y.iloc[valid_idx], y_pre_test)\n",
    "                print(\"Test Confusion Matrix\")\n",
    "                print(cm_test)\n",
    "                print(\"TesT Acc : {}\".format((cm_test[0,0] + cm_test[1,1])/cm_test.sum()))\n",
    "                print(\"Test F1-Score : {}\".format(f1_score(Y.iloc[valid_idx], y_pre_test)))\n",
    "                print(\"-----------------------------------------------------------------------\")\n",
    "                print(\"-----------------------------------------------------------------------\")\n",
    "                save_n.append(n)\n",
    "                save_l.append(l)\n",
    "                save_m.append(m)\n",
    "                save_L1.append(L1)\n",
    "                f1_score_.append(f1_score(Y.iloc[valid_idx], y_pre_test))\n",
    "\n",
    "\n",
    "                #joblib.dump(model, './LightGBM_model/Result_{}_{}_{}_{}_{}.pkl'.format(n, l, m, L1, round(save_acc[-1], 4)))\n",
    "                #gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ce86eb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> 7 <<<\n",
      "Best Test f1-score : 0.8672566371681417\n",
      "Best n_estimators : 5\n",
      "Best Learning Rate : 0.3\n",
      "Best Max_depth : 3\n",
      "Best L1-norm : 0.3\n"
     ]
    }
   ],
   "source": [
    "print(\">>> {} <<<\\nBest Test f1-score : {}\\nBest n_estimators : {}\\nBest Learning Rate : {}\\nBest Max_depth : {}\\nBest L1-norm : {}\".format(np.argmax(f1_score_),\n",
    "                                                                                                                                            f1_score_[np.argmax(f1_score_)], \n",
    "                                                                                                                                            save_n[np.argmax(f1_score_)],\n",
    "                                                                                                                                            save_l[np.argmax(f1_score_)],\n",
    "                                                                                                                                            save_m[np.argmax(f1_score_)],\n",
    "                                                                                                                                            save_L1[np.argmax(f1_score_)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e48b0029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] [cross_entropy:Init]: (objective) labels passed interval [0, 1] check\n",
      "[LightGBM] [Info] [cross_entropy:Init]: (metric) labels passed interval [0, 1] check\n",
      "[LightGBM] [Info] [cross_entropy:Init]: sum-of-weights = 372.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000656 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 905\n",
      "[LightGBM] [Info] Number of data points in the train set: 372, number of used features: 20\n",
      "[LightGBM] [Info] [cross_entropy:BoostFromScore]: pavg = 0.352151 -> initscore = -0.609600\n",
      "[LightGBM] [Info] Start training from score -0.609600\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Train Confusion Matrix\n",
      "[[228  13]\n",
      " [ 19 112]]\n",
      "Train Acc : 0.9139784946236559\n",
      "Train F1-Score : 0.875\n",
      "Test Confusion Matrix\n",
      "[[96 14]\n",
      " [ 1 49]]\n",
      "TesT Acc : 0.90625\n",
      "Test F1-Score : 0.8672566371681417\n"
     ]
    }
   ],
   "source": [
    "best_model = LGBMClassifier(n_estimators=save_n[np.argmax(f1_score_)], learning_rate=save_l[np.argmax(f1_score_)], \n",
    "                           max_depth=save_m[np.argmax(f1_score_)], reg_alpha=save_L1[np.argmax(f1_score_)], objective='cross_entropy', \n",
    "                           random_state=119)\n",
    "best_model.fit(X.iloc[train_idx], Y.iloc[train_idx])\n",
    "\n",
    "# Train Acc\n",
    "y_pre_train = best_model.predict(X.iloc[train_idx])\n",
    "cm_train = confusion_matrix(Y.iloc[train_idx], y_pre_train)\n",
    "print(\"Train Confusion Matrix\")\n",
    "print(cm_train)\n",
    "print(\"Train Acc : {}\".format((cm_train[0,0] + cm_train[1,1])/cm_train.sum()))\n",
    "print(\"Train F1-Score : {}\".format(f1_score(Y.iloc[train_idx], y_pre_train)))\n",
    "\n",
    "# Test Acc\n",
    "y_pre_test = best_model.predict(X.iloc[valid_idx])\n",
    "cm_test = confusion_matrix(Y.iloc[valid_idx], y_pre_test)\n",
    "print(\"Test Confusion Matrix\")\n",
    "print(cm_test)\n",
    "print(\"TesT Acc : {}\".format((cm_test[0,0] + cm_test[1,1])/cm_test.sum()))\n",
    "print(\"Test F1-Score : {}\".format(f1_score(Y.iloc[valid_idx], y_pre_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "00b6fa05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Score  Feature\n",
      "0      10    event\n",
      "1       4    cd496\n",
      "2       4    cd420\n",
      "3       4     cd40\n",
      "4       3  preanti\n",
      "5       2     wtkg\n",
      "6       2     race\n",
      "7       1      z30\n",
      "8       1      age\n",
      "9       0  symptom\n",
      "10      0    strat\n",
      "11      0     str2\n",
      "12      0        r\n",
      "13      0   oprior\n",
      "14      0   karnof\n",
      "15      0     homo\n",
      "16      0     hemo\n",
      "17      0   gender\n",
      "18      0    drugs\n",
      "19      0    cd820\n",
      "20      0     cd80\n"
     ]
    }
   ],
   "source": [
    "feature_map = pd.DataFrame(sorted(zip(best_model.feature_importances_, X.columns), reverse=True), columns=['Score', 'Feature'])\n",
    "print(feature_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7a5d60ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Importance Score Top 10\n",
    "feature_map_10 = feature_map.iloc[:10]\n",
    "plt.figure(figsize=(20, 10))\n",
    "sns.barplot(x=\"Score\", y=\"Feature\", data=feature_map_10.sort_values(by=\"Score\", ascending=False), errwidth=40)\n",
    "plt.title('XGBoost Importance Features')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
